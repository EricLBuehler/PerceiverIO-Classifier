{"cells":[{"cell_type":"markdown","source":["# **PerceiverIO Classifier**\n","# Eric Buehler 2022"],"metadata":{"id":"HSELM-L1rulD"},"id":"HSELM-L1rulD"},{"cell_type":"markdown","source":["#Import libraries and download perceiver library from GitHub\n","#Setup enviornment for training"],"metadata":{"id":"6gPXnkJSsMR3"},"id":"6gPXnkJSsMR3"},{"cell_type":"markdown","source":["Import libraries"],"metadata":{"id":"L7OzPytQtyow"},"id":"L7OzPytQtyow"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f9af61c"},"outputs":[],"source":["import torch\n"," \n","import torchvision\n","\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ExponentialLR, StepLR\n","\n","from torch.utils.data import DataLoader,Dataset\n","from sklearn.model_selection import train_test_split\n","\n","from PIL import Image\n","\n","import os,sys\n","import numpy as np\n","import tqdm\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","\n","to_pil = transforms.ToPILImage()\n","\n","print(\"Torch version:\", torch.__version__) "],"id":"4f9af61c"},{"cell_type":"markdown","source":["Display information on current GPU"],"metadata":{"id":"cGmgScZ9t13j"},"id":"cGmgScZ9t13j"},{"cell_type":"code","source":["!nvidia-smi -L"],"metadata":{"id":"r14yX91jsEes"},"id":"r14yX91jsEes","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi "],"metadata":{"id":"PkwaBxePsJE4"},"id":"PkwaBxePsJE4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setup enviornment and install perciever library\n","\n","Note: This block will create  \"Colab Notebooks/PercieverIO_Classifier\" directory in your Google Drive."],"metadata":{"id":"hxkSETc1toTM"},"id":"hxkSETc1toTM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dfbe6d3"},"outputs":[],"source":["modelname=\"2_4_22_m1\"\n","\n","\n","drive.mount('/content/drive')\n","\n","prefix='/content/drive/MyDrive/Colab Notebooks/PercieverIO_Classifier/'\n","\n","try:\n","    os.mkdir(prefix)\n","except FileExistsError:\n","    pass\n","\n","prefix_images=prefix+'images/'\n","try:\n","    os.mkdir(prefix_images)\n","except FileExistsError:\n","    pass\n","try:\n","    os.mkdir(prefix_images+modelname+\"/\")\n","except FileExistsError:\n","    pass\n","\n","prefix_models=prefix+'models/'+modelname+\"/\"\n","try:\n","    os.mkdir(prefix+'models/')\n","except FileExistsError:\n","    pass\n","try:\n","    os.mkdir(prefix_models)\n","except FileExistsError:\n","    pass\n","\n","CPUonly=False"],"id":"5dfbe6d3"},{"cell_type":"code","source":["!git clone https://github.com/lucidrains/perceiver-pytorch\n","!cd perceiver-pytorch/\n","!pip install perceiver-pytorch\n","\n","os.chdir(prefix)"],"metadata":{"id":"ppxZuNKG2O3A"},"id":"ppxZuNKG2O3A","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from perceiver_pytorch import Perceiver\n","from perceiver_pytorch import PerceiverIO"],"metadata":{"id":"4BM4aMJdtwZ3","executionInfo":{"status":"ok","timestamp":1644012644798,"user_tz":300,"elapsed":24,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"id":"4BM4aMJdtwZ3","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#Important variables\n","\n","\n"],"metadata":{"id":"dh3oUopLstrj"},"id":"dh3oUopLstrj"},{"cell_type":"code","execution_count":7,"metadata":{"id":"f7439325","executionInfo":{"status":"ok","timestamp":1644012644798,"user_tz":300,"elapsed":18,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"outputs":[],"source":["im_res=28 #Image resolution\n","numchannel=1 #Input image channels, 1 because 28x28x1 image\n","dim_=1 #Ouput image channels, 1 because this is classification\n","\n","autoload=False #Autoload selection\n","\n","batch_size_test = 1000 #Batch size for test\n","batch_size_train = 64 #Batch size for train"],"id":"f7439325"},{"cell_type":"markdown","source":["#Setup dataloaders, display sample image, and define other key variables"],"metadata":{"id":"2xNeLzGstJbV"},"id":"2xNeLzGstJbV"},{"cell_type":"markdown","source":["Setup dataloaders"],"metadata":{"id":"rrV7XS7Bu62X"},"id":"rrV7XS7Bu62X"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a890381f"},"outputs":[],"source":["\n","trainloader = torch.utils.data.DataLoader(\n","  torchvision.datasets.MNIST(prefix+'/files/', train=True, download=True,\n","                             transform=torchvision.transforms.Compose([\n","                               torchvision.transforms.ToTensor(),\n","                               torchvision.transforms.Normalize(\n","                                 (0.1307,), (0.3081,))\n","                             ])),\n","  batch_size=batch_size_train, shuffle=True)\n","\n","testloader = torch.utils.data.DataLoader(\n","  torchvision.datasets.MNIST(prefix+'/files/', train=False, download=True,\n","                             transform=torchvision.transforms.Compose([\n","                               torchvision.transforms.ToTensor(),\n","                               torchvision.transforms.Normalize(\n","                                 (0.1307,), (0.3081,))\n","                             ])),\n","  batch_size=batch_size_test, shuffle=True)\n","\n","print (\"Number of training batches: \", len(trainloader), \"batch size= \", batch_size_train, \"total: \",len(trainloader)*batch_size_train )\n","print (\"Number of test batches: \", len(testloader), \"batch size= \", batch_size_test, \"total: \",len(testloader)*batch_size_test)\n","\n","print(\"TOTAL images (account for full batches): \", len(trainloader)*batch_size_train+len(testloader)*batch_size_train )\n"," \n","nclasses=len(trainloader.dataset.classes)\n","print(\"Total classes: \",nclasses)"],"id":"a890381f"},{"cell_type":"markdown","source":["Display sample image"],"metadata":{"id":"_5Zjfbuju3qr"},"id":"_5Zjfbuju3qr"},{"cell_type":"code","source":["input=next(iter(testloader))[0]\n","image = to_pil(input[0])\n","print(image.size)\n","plt.imshow(image,cmap='gray')\n","plt.show()"],"metadata":{"id":"aESmdliOvrtt"},"id":"aESmdliOvrtt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define device"],"metadata":{"id":"mLVa4y7nvFjz"},"id":"mLVa4y7nvFjz"},{"cell_type":"code","execution_count":10,"metadata":{"id":"9784442f","executionInfo":{"status":"ok","timestamp":1644012647645,"user_tz":300,"elapsed":22,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if CPUonly == True:\n","     print (\"Using CPU.\")\n","     device = torch.device(\"cpu\")"],"id":"9784442f"},{"cell_type":"markdown","source":["#Autoload/define model and setup criterion, optimizer, and scheduler"],"metadata":{"id":"b8BfS4H6vIdg"},"id":"b8BfS4H6vIdg"},{"cell_type":"markdown","source":["Autoload/define model"],"metadata":{"id":"cHdy9rWLvRut"},"id":"cHdy9rWLvRut"},{"cell_type":"code","execution_count":11,"metadata":{"id":"KR40BgOZZuBF","executionInfo":{"status":"ok","timestamp":1644012657547,"user_tz":300,"elapsed":9923,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"outputs":[],"source":["directory=os.listdir(prefix_models)\n","\n","maxepoch=0\n","for item in directory:\n","    if \"txt\" in item:\n","        continue\n","    num=int(item.split(\".\")[0].split(\"_\")[2])\n","    if num>maxepoch:\n","        maxepoch=num\n","\n","\n","if autoload:\n","    if maxepoch==0:\n","        startepoch=1\n","    else:\n","        startepoch=maxepoch\n","    model=torch.load(f\"{prefix_models}model_epoch_{startepoch-1}.pth\")\n","if not autoload:\n","    startepoch=0\n","    \n","    model = PerceiverIO(\n","        dim = im_res*im_res*numchannel,                    # dimension of sequence to be encoded\n","        queries_dim = 32,            # dimension of decoder queries\n","        logits_dim = nclasses,            # dimension of final logits\n","        depth = 6,                   # depth of net\n","        num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n","        latent_dim = 512,            # latent dimension\n","        cross_heads = 1,             # number of heads for cross attention. paper said 1\n","        latent_heads = 8,            # number of heads for latent self attention, 8\n","        cross_dim_head = 64,         # number of dimensions per cross attention head\n","        latent_dim_head = 64,        # number of dimensions per latent self attention head\n","        weight_tie_layers = False    # whether to weight tie layers (optional, as indicated in the diagram)\n","    )\n","    model.to(device)"],"id":"KR40BgOZZuBF"},{"cell_type":"markdown","source":["Define criterion, optimizer, and scheduler"],"metadata":{"id":"06yaX6WDvWi3"},"id":"06yaX6WDvWi3"},{"cell_type":"code","execution_count":12,"metadata":{"id":"9815bc1a","executionInfo":{"status":"ok","timestamp":1644012657547,"user_tz":300,"elapsed":19,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"outputs":[],"source":["criterion =  nn.MSELoss()\n","optimizer = optim.Adam(model.parameters() , lr=0.0001 )\n","scheduler = ExponentialLR(optimizer, gamma=0.9)"],"id":"9815bc1a"},{"cell_type":"markdown","source":["Define queries"],"metadata":{"id":"gcwFt7j84c90"},"id":"gcwFt7j84c90"},{"cell_type":"code","source":["queries=torch.zeros(1,32)\n","queries=queries.to(device)"],"metadata":{"id":"Q7ki61DO4YUz","executionInfo":{"status":"ok","timestamp":1644012657548,"user_tz":300,"elapsed":18,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"id":"Q7ki61DO4YUz","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#Utility functions"],"metadata":{"id":"m5iD-YMOvceC"},"id":"m5iD-YMOvceC"},{"cell_type":"markdown","source":["Unflatten"],"metadata":{"id":"zT2Z9nLyveYW"},"id":"zT2Z9nLyveYW"},{"cell_type":"code","execution_count":14,"metadata":{"id":"09810e75","executionInfo":{"status":"ok","timestamp":1644012657548,"user_tz":300,"elapsed":17,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"outputs":[],"source":["m2 = nn.Sequential(\n","  nn.Unflatten (1, (dim_, im_res, im_res))\n",")"],"id":"09810e75"},{"cell_type":"markdown","source":["Flattening function"],"metadata":{"id":"aDB-dMU6vfzX"},"id":"aDB-dMU6vfzX"},{"cell_type":"code","execution_count":15,"metadata":{"id":"3d57ffba","executionInfo":{"status":"ok","timestamp":1644012657549,"user_tz":300,"elapsed":18,"user":{"displayName":"Eric Buehler","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4qAlgAbRUkIi03Xh0PBfsvustfm71WbVsJ3JNDw=s64","userId":"08622126153041419672"}}},"outputs":[],"source":["def conv_flattened_to_image_ind (inputs):\n","    outputs_for_image= torch.clone  (inputs)\n","    outputs_for_image=torch.permute(outputs_for_image, (0,2,1)  )\n","    outputs_for_image=torch.flatten(outputs_for_image, start_dim=1, end_dim=2)\n","    outputs_for_image = m2(outputs_for_image)\n","\n","    return outputs_for_image\n"],"id":"3d57ffba"},{"cell_type":"markdown","source":["#Train"],"metadata":{"id":"gVKbnMbnvpcM"},"id":"gVKbnMbnvpcM"},{"cell_type":"markdown","source":["Train the model.\n","Model autosaves and displays plentiful information"],"metadata":{"id":"ScnGIL4avrEP"},"id":"ScnGIL4avrEP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"79aa3c7b","scrolled":false,"cellView":"code"},"outputs":[],"source":["#@title Default title text\n","lowestloss = 1000\n","\n","epochs = 50\n","numimgs = 8\n","steps = 0\n","print_every  = len (trainloader)\n","running_loss = 0.0\n","\n","torch.cuda.empty_cache()\n","\n","for epoch in range(startepoch, epochs):\n","    train_losses_epoch, test_losses_epoch, accuracy, val_acc = [],[], [], []\n","\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    for inputs, labels in tqdm.tqdm(trainloader):\n","        steps += 1\n","\n","        #Perpute input\n","        inputs=torch.permute(inputs, (0,2,3,1))\n","        \n","        #One-hot encode labels\n","        labels=F.one_hot(labels,num_classes=nclasses)\n","        labels.unsqueeze_(1)\n","\n","        #Reset gradients\n","        optimizer.zero_grad()\n","        \n","        #Flatten inputs\n","        inputs=torch.flatten(inputs, start_dim=1, end_dim=3)\n","        inputs.unsqueeze_(1)\n","        \n","        #Move inputs/labels to device\n","        inputs,labels = inputs.to(device),labels.to(device)\n","\n","        #Run model, with inputs and queries as input\n","        outputs=model(inputs,queries=queries )\n","\n","        #Get loss\n","        loss = criterion(outputs.float(), labels.float() )\n","        \n","        #Backpropogate loss\n","        loss.backward()\n","\n","        #Step optimizer\n","        optimizer.step()\n","\n","        train_losses_epoch.append(loss.item())\n","\n","\n","    #Set model into eval mode\n","    model.eval()\n","    \n","    print(\"\\nNow evaluate test batches...\")\n","    with torch.no_grad():\n","        for inputs, labels  in testloader:\n","            #Clone labels\n","            labels_=labels.clone()\n","            #Permute input\n","            inputs=torch.permute(inputs, (0,2,3,1)  )\n","            \n","            #One-hot encode labels\n","            labels=F.one_hot(labels,num_classes=nclasses)\n","            labels.unsqueeze_(1)\n","\n","            #Reset gradients\n","            optimizer.zero_grad()\n","            \n","            #Flatten inputs\n","            inputs=torch.flatten(inputs, start_dim=1, end_dim=3)\n","            inputs.unsqueeze_(1)\n","\n","            #Move inputs/labels/labels_ to device\n","            inputs,labels,labels_ = inputs.to(device),labels.to(device),labels_.to(device)\n","\n","            #Run model, with inputs and queries as input\n","            outputs=model(inputs,queries=queries )\n","\n","            #Calculate test loss\n","            outputs_=outputs.argmax(-1).cpu().detach().numpy()\n","            batch_loss =   criterion(outputs.float(), labels.float() ) #estimate loss for test batch\n","\n","            test_losses_epoch.append(batch_loss.item())\n","                \n","\n","            \n","            #Display images\n","            numr=numimgs-3 #  columns\n","\n","            #Unflatten the data, make it into an image array\n","            outputs_for_image_inputs=conv_flattened_to_image_ind(inputs)\n","            fig=plt.figure(figsize=(8,2*numimgs ))\n","            imcount=0\n","\n","            #ii_ is counter of images drawn from batch\n","            ii_=0\n","            while ii_<numimgs: \n","                print (\"Image # from batch considered: \", ii_)\n","                \n","                image = to_pil( outputs_for_image_inputs[ii_,:]  )\n","\n","                print(f\"Predicted class: {outputs_[ii_][0]}\")\n","                print(f\"Real class: {labels_[ii_]}\")\n","                sub = fig.add_subplot(numr, 3 ,   imcount +1)\n","                sub.set_title(f\"Image sample\\nPredicted class: {outputs_[ii_][0]}\\nReal class: {labels_[ii_]}\" )\n","                plt.axis('off')\n","                plt.imshow(image,cmap='gray')\n","                \n","                imcount=imcount+1\n","\n","                ii_=ii_+1 #Next image within test batch\n","                \n","            \n","            plt.savefig(prefix_images+f\"{modelname}/image_epoch_{epoch}.png\")\n","            plt.show()\n","\n","            #Calculate accuracy\n","            ps = torch.exp(outputs.squeeze()) #Squeeze used to remove dimension 1\n","            top_p, top_class = ps.topk(1, dim=1)\n","            equals = top_class == labels_.view(*top_class.shape)\n","\n","            accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n","            \n","            break\n","            \n","                \n","    print(\"Evaluation of test images done.\")\n","    val_acc.append(sum(accuracy)/len(accuracy)) \n","        \n","    print(f\"Epoch {epoch+1}/{epochs}\\n\"\n","            f\"Train loss: {sum(train_losses_epoch)/len(train_losses_epoch):.6f}\\n \"\n","            f\"Test loss: {sum(test_losses_epoch)/len(test_losses_epoch):.6f}\\n \"\n","            f\"Accuracy: {sum(accuracy)/len(accuracy):.6f}\"\n","            )\n","\n","    #Set model into train mode\n","    model.train()\n","\n","\n","    #Save model for current epoch\n","    namesve = prefix_models+f\"model_epoch_{epoch}.pth\"\n","    torch.save(model, namesve)\n","\n","    #Save losses and accuracy\n","    with open(prefix_models+\"train_loss.txt\",\"a\") as file:\n","        for item in train_losses_epoch:\n","            file.write(f\"E{epoch}_{item}\\n\")\n","\n","    with open(prefix_models+\"test_loss.txt\",\"a\") as file:\n","        for item in test_losses_epoch:\n","            file.write(f\"E{epoch}_{item}\\n\")\n","    \n","    with open(prefix_models+\"accuracy.txt\",\"a\") as file:\n","        for item in val_acc:\n","            file.write(f\"E{epoch}_{item}\\n\")\n","\n","    #Step scheduler\n","    scheduler.step()"],"id":"79aa3c7b"},{"cell_type":"markdown","source":["Save final model"],"metadata":{"id":"e8ZsyHXA97Mt"},"id":"e8ZsyHXA97Mt"},{"cell_type":"code","source":["namesve = prefix_models+f\"model_final.pth\"\n","torch.save(model, namesve)\n","\n","print('Finished Training')"],"metadata":{"id":"7D6Rb75m9y5J"},"id":"7D6Rb75m9y5J","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train.ipynb","provenance":[{"file_id":"1-I6SSVT53_cPJG8zuzgm8_BEm6VXvATI","timestamp":1642727311650}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}